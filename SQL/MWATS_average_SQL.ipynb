{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Impact,Arial;font-size:50px\">MWATS Statistics </h1>\n",
    "<p> These data were extracted from the Variable and Slow Transients (VAST) pipeline in csv format and then converted to parquet format via the Load.ipynb script. In this notebook we apply filters and average the data on a daily cadence. We then generate a statistics table to explore the data. Some example plots have been included at the end of the script to plot locations of highly variable objects as well as exploring the overall statistics of the dataset. For more information contact: Martin.Bell@uts.edu.au.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Columns\n",
    "### _data_ table:\n",
    "\n",
    "- **raw_peak_flux**: flux density [_JY - Janskys_] (or brightness of the observation).\n",
    "\n",
    "- **source_name**  : unique identifier of a given source (or object). \n",
    "\n",
    "- **jd**          : Julian date [_D - days_].\n",
    "\n",
    "- **ra** : Right Ascension [_Deg - Degrees_].\n",
    "\n",
    "- **dec** : Declination [_Deg - Degrees_].\n",
    "\n",
    "- **im_ra**: Image Right Ascension [_Deg - Degrees_] i.e. the RA of the pointing centre of the image from which the measurement was taken. \n",
    "\n",
    "- **im_dec**: Image Declination [_Deg - Degrees_] i.e. the Dec of the pointing centre of the image from which the measurement was taken. \n",
    "\n",
    "- **distance**: Distance [_Deg - Degrees_]: distance between pointing centre and flux measurement. \n",
    "\n",
    "- **gain**: Gain [no units]. Multiplicative factor that has been applied to each image (and flux measurements in image) to correct flux scale. Should be close to 1. \n",
    "\n",
    "### _data _avg_ table: \n",
    "\n",
    "- **median_flux** : median flux density [_JY - Janskys_] of daily measurements. \n",
    "\n",
    "- **std_flux** : standard deviation of daily flux measurements [_JY - Janskys_].\n",
    "\n",
    "- **mean_jd** : mean Julian date of daily measurements [_D - days_]. \n",
    "\n",
    "- **datetime** : pandas datetime object of mean date of observations. \n",
    "\n",
    "\n",
    "### _stats_ table: \n",
    "\n",
    "- **mean_raw_peak_flux** : mean flux density of all measurements [_JY - Janskys_].\t\n",
    "\n",
    "- **std_raw_peak_flux**\t : standard deviation of all measurements [_JY - Janskys_].\n",
    "\n",
    "- **Mod** Modulation Index [%].\t\n",
    "\n",
    "- **length**: Length of time series [_N_].\n",
    "- **sig**: Significance of gradient [grad / fit_error].\t\n",
    "- **grad** : Linear regression fit to the data.\t\n",
    "- **fit_error** : Error on the linear regression. NOTE: error bars were not used in this fit as per Bell et al. 2018, only the scatter on the measurements. \t\n",
    "- **y_int** : Y intercept of the linear fit.\n",
    "\n",
    "- **avg_sig**: Significance of gradient [grad / fit_error] for daily median flux measurements.\t\n",
    "- **avg_grad** : Linear regression fit to the data for daily median flux measurements.\t\n",
    "\n",
    "- **avg_fit_error** : Error on the linear regression for daily median flux measurements. NOTE: error bars were not used in this fit as per Bell et al. 2018, only the scatter on the measurements. \t\n",
    "\n",
    "- **avg_y_int** : Y intercept of the linear fit for daily median flux measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from scipy import stats\n",
    "from scipy.stats import linregress, entropy\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews.operation import decimate\n",
    "from holoviews.operation.datashader import datashade, rasterize\n",
    "hv.extension('bokeh')\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "from datashader.colors import inferno\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from astropy.coordinates import ICRS, Galactic, FK4, FK5\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import time\n",
    "\n",
    "import nmslib;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_from_median_pos(df):\n",
    "    degrees_to_radians = np.pi/180.0\n",
    "    phi1 = df.dec*degrees_to_radians\n",
    "    phi2 = df.median_dec*degrees_to_radians\n",
    "\n",
    "    theta1 = df.ra*degrees_to_radians\n",
    "    theta2 = df.median_ra*degrees_to_radians\n",
    "    \n",
    "    cosine = (np.cos(phi1)*np.cos(phi2)*np.cos(theta1 - theta2) +\n",
    "           np.sin(phi1)*np.sin(phi2))\n",
    "    dist_from_centre = np.arccos(cosine)\n",
    "    return (dist_from_centre/3.142)*180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "The input data file is the feather file created in Load.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = pd.read_feather('mwats_raw_data_Mar_SQL.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Add in columns for median RA/DEC and flux.\n",
    "tmp3 = data.groupby('source_id')[['ra','dec','raw_peak_flux']].median().reset_index() \n",
    "tmp3.columns = ['source_id', 'median_ra', 'median_dec', 'median_flux']\n",
    "data = pd.merge(data,\n",
    "                 tmp3[['source_id', 'median_ra', 'median_dec', 'median_flux']],\n",
    "                 on='source_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Add in columns for std RA/DEC and flux.\n",
    "tmp4 = data.groupby('source_id')[['ra','dec']].std().reset_index() \n",
    "tmp4.columns = ['source_id', 'std_ra', 'std_dec']\n",
    "data = pd.merge(data,\n",
    "                 tmp4[['source_id', 'std_ra', 'std_dec']],\n",
    "                 on='source_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Add offet and flux_offset data columns. \n",
    "# offset = distance (in deg) from median position. \n",
    "# flux_offset = measured flux minus the median\n",
    "data['offset'] = distance_from_median_pos(data)\n",
    "data['flux_offset'] = np.sqrt((data.raw_peak_flux-data.median_flux)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering: \n",
    "Below are the filters that have been applied to the data. These can be modified based on the users requirement. \n",
    "\n",
    "- _Distance from pointing centre_: We filter measurements less than 12 degrees from the pointing centre as per Bell et al. 2016, 2018. Before filtering we have 10 million flux measurements and after filtering 6.2 million. It is quite subjective to define the correct distance, but 12 degrees has worked well in the past.  \n",
    "\n",
    "- _max__flux_: The brightest object to consider in the analysis. Objects > 50 Jy are typically so bright they cause problems (in imaging) and are hard to generate reliable light-curves. \n",
    "\n",
    "- _min__length_: The minimum nimber of points to consider in the light-curves. Objects with small numbers of measurements may be spurius and unrealiable.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters (some are applied later in the script)\n",
    "max_dist   = 15.0 # Only use measurements less that 12-15 degrees from pointing centre of the image.\n",
    "max_flux   = 50.0 # Jy. Only consider sources with a maximum flux of max_flux.\n",
    "min_flux   = 0.25 # Jy. Only consider sources with a minimum flux of min_flux.\n",
    "min_length = 35 # Minimum length of time-series to consider. \n",
    "max_rms    = 500 # Remove images with RMS greater than this. A few RFI soaked images crept threw. \n",
    "min_rms    = 25  # A couple of badly calibrated images need to be removed. \n",
    "max_offset = 0.0375 # beam = 3 pixels with width @ 0.75' per pixel (0.75*3)/60 = 0.0375\n",
    "max_flux_offset = 5000 # flux_offset = |flux - median_flux| = Gets ride of small number of bad fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply distance from pointing centre filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.distance < max_dist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply rms image filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.rms < max_rms]\n",
    "data = data[data.rms > min_rms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply position offset filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data points more than 3 pixels away from median position\n",
    "data = data[data.offset < max_offset] # beam = 3 pixels with width @ 0.75' per pixel (0.75*3)/60 = 0.0375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply flux offset filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove sources with really large flux offsets. They are just junk.\n",
    "data = data[data.flux_offset<max_flux_offset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove sources that have more than one measurement in an image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sources = (data\n",
    "                .groupby(['image_id','source_id'])\n",
    "                .size().rename('count').to_frame()\n",
    "                .reset_index().query('count == 1')['source_id']\n",
    "               ).values\n",
    "data = data[data.source_id.isin(good_sources)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the date as the index\n",
    "data.set_index('time', inplace=True)\n",
    "\n",
    "# Remove sources without as source_id\n",
    "data = data[data.source_id.notnull()]\n",
    "\n",
    "# Convert source_id to int\n",
    "data['source_id'] = data.source_id.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging the full dataset\n",
    "The steps below re-sample the raw data on day timescales.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below speeds up the computation of the averaged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "data_avg = (data.groupby(['source_id', pd.Grouper(freq = '1d')])\n",
    "                .agg({\n",
    "                        'raw_peak_flux': ['median', 'std'],\n",
    "                        'jd': 'mean'\n",
    "                     })\n",
    "                .dropna()\n",
    "           )\n",
    "\n",
    "data_avg.columns = data_avg.columns.map('_'.join)\n",
    "\n",
    "data_avg = (data_avg\n",
    "                .rename(columns={\n",
    "                    'raw_peak_flux_median': 'median_flux',\n",
    "                    'raw_peak_flux_std': 'std_flux',\n",
    "                    'jd_mean': 'mean_jd'\n",
    "                                })\n",
    "                .reset_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate non-averaged statistics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate a new stats table with the mean flux of the non-averaging sources\n",
    "stats = data.groupby('source_id')[['ra','dec','raw_peak_flux']].mean().reset_index() \n",
    "stats.columns = ['source_id', 'ra', 'dec', 'mean_raw_peak_flux']\n",
    "\n",
    "# Maybe to replace repeated uses\n",
    "#flux_by_source = data.groupby('source_id')['raw_peak_flux']\n",
    "\n",
    "# Stats: std\n",
    "tmp = data.groupby('source_id')['raw_peak_flux'].std(ddof=1).reset_index()\n",
    "tmp.columns = ['source_id', 'std_raw_peak_flux']\n",
    "stats['std_raw_peak_flux'] = tmp['std_raw_peak_flux']\n",
    "\n",
    "# Do some basic filtering of df to get rid of junky data points\n",
    "stats = stats[(stats.mean_raw_peak_flux > 0) & (stats.std_raw_peak_flux > 0)]\n",
    "\n",
    "# Stats: modulation index\n",
    "stats['Mod'] = (stats.std_raw_peak_flux / stats.mean_raw_peak_flux) * 100 # In percent\n",
    "\n",
    "# Stats: number of measurements per source. \n",
    "tmp2 = data.groupby('source_id')['raw_peak_flux'].count().reset_index()\n",
    "tmp2.columns = ['source_id', 'length']\n",
    "stats['length'] = tmp2['length']\n",
    "\n",
    "\n",
    "'''\n",
    "# Stats: Flux variability relative to noise \n",
    "def inversesqr(x):\n",
    "    return 1./(x*2)\n",
    "tmp3 = data.groupby('source_id')['rms'].apply(inversesqr)\n",
    "tmp3.columns = ['source_id','weight']\n",
    "tmp4 = data.groupby('source_id')['raw_peak_flux']\n",
    "tmp4.columns = ['source_id','flux']\n",
    "\n",
    "\n",
    "# Look for outliers\n",
    "# warning quantiles are slow!\n",
    "def q50(x):\n",
    "    return x.quantile(.50)\n",
    "def q16(x):\n",
    "    return x.quantile(.1586)\n",
    "def q84(x):\n",
    "    return x.quantile(.8414)\n",
    "f = {'raw_peak_flux': [q16, q50, q84]}\n",
    "tmp = data.groupby('source_id').agg(f)\n",
    "tmp_q50 = data.groupby('source_id')['raw_peak_flux'].quantile(.50).reset_index()\n",
    "tmp_q50.columns = ['source_id', 'q50_raw_peak_flux']\n",
    "\n",
    "tmp_flux = tmp.reset_index()['raw_peak_flux']\n",
    "tmp_q50 = tmp_flux['q50']\n",
    "tmp_iqr = tmp_flux['q84'] - tmp_flux['q16']\n",
    "data['is_flux_high'] = data['raw_peak_flux'].sub(tmp_q50).div(tmp_iqr)  > 3.0\n",
    "#data['is_flux_high'] = data.groupby('source_id')['scaled_flux'].transform(lambda x: x > 3.0)\n",
    "stats['frac_high'] = data.groupby('source_id')['is_flux_high'].sum() / stats['length']\n",
    "'''\n",
    "\n",
    "# Do some astronomy based filtering\n",
    "stats = stats[(stats.mean_raw_peak_flux > min_flux)\n",
    "     & (stats.mean_raw_peak_flux < max_flux)\n",
    "     & (stats.length > min_length)]\n",
    "\n",
    "filtered_raw_data = data[data.source_id.isin(stats.source_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  Calculate the gradient (non-averaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get the dates and fluxes\n",
    "dates  = filtered_raw_data.groupby('source_id')['jd'].apply(list)\n",
    "fluxes = filtered_raw_data.groupby('source_id')['raw_peak_flux'].apply(list)\n",
    "\n",
    "# Concat time and flux and do the fit\n",
    "time_flux = pd.concat((dates, fluxes), axis=1)\n",
    "grad_fit = time_flux.apply(lambda x: linregress(x['jd'], x['raw_peak_flux']), axis=1)\n",
    "\n",
    "# Calculate all of the terms\n",
    "grad = grad_fit.map(lambda x: x.slope)\n",
    "fit_error = grad_fit.map(lambda x: x.stderr)\n",
    "sig = grad_fit.map(lambda l: l.slope/l.stderr)\n",
    "y_int = grad_fit.map(lambda x: x.intercept)\n",
    "\n",
    "stats = stats.assign(sig=sig.values)\n",
    "stats = stats.assign(grad=grad.values)\n",
    "stats = stats.assign(fit_error=fit_error.values)\n",
    "stats = stats.assign(y_int=y_int.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate averaged statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to work with the sources in our filtered stats table (i.e. have decent sized light-curves).\n",
    "filtered_raw_avg_data = data_avg[data_avg.source_id.isin(stats.source_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get the dates and fluxes\n",
    "dates  = filtered_raw_avg_data.groupby('source_id')['mean_jd'].apply(list)\n",
    "fluxes = filtered_raw_avg_data.groupby('source_id')['median_flux'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Concat time and flux and do the fit\n",
    "time_flux = pd.concat((dates, fluxes), axis=1)\n",
    "grad_fit = time_flux.apply(lambda x: linregress(x['mean_jd'], x['median_flux']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate all of the terms and add them to the stats table\n",
    "avg_grad = grad_fit.map(lambda x: x.slope)\n",
    "avg_fit_error = grad_fit.map(lambda x: x.stderr)\n",
    "avg_sig = grad_fit.map(lambda l: l.slope/l.stderr)\n",
    "avg_y_int = grad_fit.map(lambda x: x.intercept)\n",
    "\n",
    "stats = stats.assign(avg_sig=avg_sig.values)\n",
    "stats = stats.assign(avg_grad=avg_grad.values)\n",
    "stats = stats.assign(avg_fit_error=avg_fit_error.values)\n",
    "stats = stats.assign(avg_y_int=avg_y_int.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.set_index('source_id', inplace=True) # The indices of stats and tmp3 and tmp4 must be aligned. Let's set them to source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate the modulation index of the daily median flux measurements. \n",
    "\n",
    "# Add in the std of all the daily resampled measurements.\n",
    "tmp3 = filtered_raw_avg_data.groupby('source_id')['median_flux'].std(ddof=1).reset_index()\n",
    "tmp3.columns = ['source_id', 'avg_std']\n",
    "tmp3.set_index('source_id', inplace=True)\n",
    "stats['avg_std'] = tmp3['avg_std']\n",
    "\n",
    "# Calculate the mean of the median daily flux measurements. \n",
    "tmp4 = filtered_raw_avg_data.groupby('source_id')['median_flux'].mean().reset_index()\n",
    "tmp4.columns = ['source_id', 'avg_median']\n",
    "tmp4.set_index('source_id', inplace=True)\n",
    "stats['avg_median'] = tmp4['avg_median']\n",
    "\n",
    "# Calculate the modulation index\n",
    "stats['avg_Mod'] = (stats.avg_std / stats.avg_median) * 100 # In percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.reset_index(inplace=True)\n",
    "filtered_raw_avg_data.reset_index(inplace=True)\n",
    "filtered_raw_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest neighbours\n",
    "\n",
    "Using https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/search_vector_dense_optim.ipynb\n",
    "with Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_positions = filtered_raw_data.groupby('source_id')[['ra', 'dec']].mean()\n",
    "source_positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source2idx = {s: i for i, s in enumerate(source_positions.index.values)}\n",
    "idx2source = {v:k for (k, v) in source2idx.items()}\n",
    "X = source_positions.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index parameters\n",
    "# These are the most important ones\n",
    "M = 15\n",
    "efC = 100\n",
    "\n",
    "num_threads = 4\n",
    "index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC, 'post' : 0}\n",
    "print('Index-time parameters', index_time_params)\n",
    "\n",
    "# Number of neighbors \n",
    "K=100\n",
    "\n",
    "# Space name should correspond to the space name \n",
    "# used for brute-force search\n",
    "space_name='l2'\n",
    "\n",
    "# Intitialize the library, specify the space, the type of the vector and add data points \n",
    "index = nmslib.init(method='hnsw', space=space_name, data_type=nmslib.DataType.DENSE_VECTOR) \n",
    "index.addDataPointBatch(X)\n",
    "\n",
    "# Create an index\n",
    "start = time.time()\n",
    "index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC}\n",
    "index.createIndex(index_time_params) \n",
    "end = time.time() \n",
    "print('Index-time parameters', index_time_params)\n",
    "print('Indexing time = %f' % (end-start))\n",
    "\n",
    "# Setting query-time parameters\n",
    "efS = 100\n",
    "query_time_params = {'efSearch': efS}\n",
    "print('Setting query-time parameters', query_time_params)\n",
    "index.setQueryTimeParams(query_time_params)\n",
    "\n",
    "query_qty = X.shape[0]\n",
    "start = time.time() \n",
    "nbrs = index.knnQueryBatch(X, k = K, num_threads = num_threads)\n",
    "end = time.time() \n",
    "print('kNN time total=%f (sec), per query=%f (sec), per query adjusted for thread number=%f (sec)' % \n",
    "      (end-start, float(end-start)/query_qty, num_threads*float(end-start)/query_qty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the neighbours array into a dataframe\n",
    "\n",
    "source_id = []\n",
    "n_id1 = []\n",
    "n_id2 = []\n",
    "n_id3 = []\n",
    "n_id4 = []\n",
    "n_id5 = []\n",
    "\n",
    "for n in nbrs:\n",
    "    tmp_id = idx2source[n[0][0]]\n",
    "    tmp_id1 = idx2source[n[0][1]]\n",
    "    tmp_id2 = idx2source[n[0][2]]\n",
    "    tmp_id3 = idx2source[n[0][3]]\n",
    "    tmp_id4 = idx2source[n[0][4]]\n",
    "    tmp_id5 = idx2source[n[0][5]]\n",
    "    \n",
    "    source_id.append(tmp_id)\n",
    "    n_id1.append(tmp_id1)\n",
    "    n_id2.append(tmp_id2)\n",
    "    n_id3.append(tmp_id3)\n",
    "    n_id4.append(tmp_id4)\n",
    "    n_id5.append(tmp_id5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = pd.DataFrame(\n",
    "    {'source_id': source_id,\n",
    "     'n_id1': n_id1,\n",
    "     'n_id2': n_id2,\n",
    "     'n_id3': n_id3,\n",
    "     'n_id4': n_id4,\n",
    "     'n_id5': n_id5,\n",
    "     'source_id_ra'   : stats.set_index('source_id').loc[source_id].ra,\n",
    "     'source_id_dec'  : stats.set_index('source_id').loc[source_id].dec,\n",
    "      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = neighbours.drop(\"source_id\", axis=1)\n",
    "neighbours = neighbours.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_id1_ra = stats.set_index('source_id').loc[n_id1].ra\n",
    "n_id1_dec = stats.set_index('source_id').loc[n_id1].dec\n",
    "n_id2_ra = stats.set_index('source_id').loc[n_id2].ra\n",
    "n_id2_dec = stats.set_index('source_id').loc[n_id2].dec\n",
    "n_id3_ra = stats.set_index('source_id').loc[n_id3].ra\n",
    "n_id3_dec = stats.set_index('source_id').loc[n_id3].dec\n",
    "n_id4_ra = stats.set_index('source_id').loc[n_id4].ra\n",
    "n_id4_dec = stats.set_index('source_id').loc[n_id4].dec\n",
    "n_id5_ra = stats.set_index('source_id').loc[n_id5].ra\n",
    "n_id5_dec = stats.set_index('source_id').loc[n_id5].dec\n",
    "\n",
    "neighbours = neighbours.assign(n_id1_ra=n_id1_ra.values)\n",
    "neighbours = neighbours.assign(n_id1_dec=n_id1_dec.values)\n",
    "neighbours = neighbours.assign(n_id2_ra=n_id2_ra.values)\n",
    "neighbours = neighbours.assign(n_id2_dec=n_id2_dec.values)\n",
    "neighbours = neighbours.assign(n_id3_ra=n_id3_ra.values)\n",
    "neighbours = neighbours.assign(n_id3_dec=n_id3_dec.values)\n",
    "neighbours = neighbours.assign(n_id4_ra=n_id4_ra.values)\n",
    "neighbours = neighbours.assign(n_id4_dec=n_id4_dec.values)\n",
    "neighbours = neighbours.assign(n_id5_ra=n_id5_ra.values)\n",
    "neighbours = neighbours.assign(n_id5_dec=n_id5_dec.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_distance_on_unit_sphere(s1_ra, s2_ra, s1_dec, s2_dec):\n",
    "    degrees_to_radians = np.pi/180.0\n",
    "    phi1 = s1_dec*degrees_to_radians\n",
    "    phi2 = s2_dec*degrees_to_radians\n",
    "\n",
    "    theta1 = s1_ra*degrees_to_radians\n",
    "    theta2 = s2_ra*degrees_to_radians\n",
    "    \n",
    "    cosine = (np.cos(phi1)*np.cos(phi2)*np.cos(theta1 - theta2) +\n",
    "           np.sin(phi1)*np.sin(phi2))\n",
    "    dist_from_centre = np.arccos(cosine)\n",
    "    return (dist_from_centre/3.142)*180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours['dist_1'] = vectorized_distance_on_unit_sphere(neighbours.source_id_ra, neighbours.n_id1_ra, neighbours.source_id_dec, neighbours.n_id1_dec)\n",
    "neighbours['dist_2'] = vectorized_distance_on_unit_sphere(neighbours.source_id_ra, neighbours.n_id2_ra, neighbours.source_id_dec, neighbours.n_id2_dec)\n",
    "neighbours['dist_3'] = vectorized_distance_on_unit_sphere(neighbours.source_id_ra, neighbours.n_id3_ra, neighbours.source_id_dec, neighbours.n_id3_dec)\n",
    "neighbours['dist_4'] = vectorized_distance_on_unit_sphere(neighbours.source_id_ra, neighbours.n_id4_ra, neighbours.source_id_dec, neighbours.n_id4_dec)\n",
    "neighbours['dist_5'] = vectorized_distance_on_unit_sphere(neighbours.source_id_ra, neighbours.n_id5_ra, neighbours.source_id_dec, neighbours.n_id5_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax=np.log10(neighbours.dist_0).hist(bins=100, figsize=(10, 5), grid=False, ec='k')\n",
    "ax=(neighbours.dist_1).hist(bins=100, figsize=(10, 5), grid=False, ec='k', label='Sources')\n",
    "plt.plot([0.0625,0.0625],[0,6000], 'r-.', label='5 pixels distance')\n",
    "plt.xlabel('Distance from nearest neighbour (deg)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out sources that are closer than 5 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the nearby sources\n",
    "close_source = neighbours[neighbours.dist_1 > 0.0625] # > 5 pixels ADD AS PARAMETER ABOVE\n",
    "stats = stats[stats.source_id.isin(close_source.source_id)] # Do stats table only, leave in place in data tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=(neighbours.dist_1).hist(bins=80, figsize=(10, 5), grid=False, histtype='step', label='Sources')\n",
    "ax=(neighbours.dist_2).hist(bins=70, figsize=(10, 5), grid=False, histtype='step', label='Sources')\n",
    "ax=(neighbours.dist_3).hist(bins=100, figsize=(10, 5), grid=False, histtype='step', label='Sources')\n",
    "ax=(neighbours.dist_4).hist(bins=90, figsize=(10, 5), grid=False, histtype='step', label='Sources')\n",
    "ax=(neighbours.dist_5).hist(bins=100, figsize=(10, 5), grid=False, histtype='step', label='Sources')\n",
    "plt.plot([0.0625,0.0625],[0,6000], 'r-.', label='5 pixels distance')\n",
    "plt.xlabel('Distance from nearest neighbour (deg)')\n",
    "#plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src in [3045]: #stats.source_id:\n",
    "    nn1 = neighbours[neighbours.source_id==src].iloc[0][\"n_id1\"]\n",
    "    print(nn1)\n",
    "    lc_src = data[data.source_id==src].raw_peak_flux \n",
    "    print(lc_src)\n",
    "    lc_nb1 = data[data.source_id==nn1].raw_peak_flux\n",
    "    #if lc_src.length != lc_nb1.length:\n",
    "    #    print(str(src)+\": LC not the same length as neighbour!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.reset_index(inplace=True)\n",
    "filtered_raw_avg_data.reset_index(inplace=True)\n",
    "filtered_raw_data.reset_index(inplace=True)\n",
    "neighbours.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.to_feather('stats_table.fth')\n",
    "filtered_raw_avg_data.to_feather('avg_data.fth')\n",
    "filtered_raw_data.to_feather('data.fth')\n",
    "neighbours.to_feather('neighbours.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
